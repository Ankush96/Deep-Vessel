{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "from skimage.util import img_as_float, img_as_ubyte\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_path(directory):\n",
    "    imgs = glob.glob(directory + '/images/*.tif')\n",
    "    #a = [x.split('/')[-1].split('.')[0] for x in train]\n",
    "    \n",
    "    mask = glob.glob(directory + '/mask/*.gif')\n",
    "    #b = [x.split('/')[-1].split('.')[0] for x in mask]\n",
    "    \n",
    "    gt = glob.glob(directory + '/1st_manual/*.gif')\n",
    "    #c = [x.split('/')[-1].split('.')[0] for x in gt]\n",
    "    \n",
    "    return map(os.path.abspath, imgs), map(os.path.abspath, mask), map(os.path.abspath, gt)\n",
    "\n",
    "train, mask_train, gt_train =  get_path('../Data/DRIVE/training')\n",
    "test, mask_test, mask_gt = get_path('../Data/DRIVE/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Hyper Params\n",
    "total_patches = 600\n",
    "num_training_images = len(train)\n",
    "patches_per_image = total_patches/num_training_images\n",
    "patch_dim = 31                         \n",
    "current_batch_ind = 0\n",
    "print patches_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../Data/mean_normalised_df.pkl') \n",
    "mean_img = pd.read_pickle('../Data/mean_img.pkl')\n",
    "def next_batch(size, df):\n",
    "    global current_batch_ind\n",
    "    flag = False\n",
    "    if current_batch_ind + size> len(df):\n",
    "        print 'Next batch cannot be called because of insufficient remaining data'\n",
    "        flag = True\n",
    "    batch_x = np.zeros((size, patch_dim**2*3))\n",
    "    batch_y = np.zeros((size,1), dtype = 'uint8')\n",
    "    for i in range(current_batch_ind, current_batch_ind+size):\n",
    "        batch_x[i - current_batch_ind] = df.loc[i][:-1]\n",
    "        batch_y[i - current_batch_ind] = int(df.loc[i][patch_dim**2*3])\n",
    "    current_batch_ind += size\n",
    "    return (batch_x, batch_y), flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7fe4d2adc390>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, W, pad_type=1):\n",
    "    if pad_type == 1:\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "def max_pool_2x2(x,pad_type=1):\n",
    "    if pad_type == 1:\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='VALID')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, patch_dim**2*3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([4, 4, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,patch_dim,patch_dim,3]) # Check\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1,0) + b_conv1)\n",
    "\n",
    "\n",
    "W_conv2 = weight_variable([4, 4, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "h_pool1 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "W_conv3 = weight_variable([4, 4, 64, 64])\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool1, W_conv3) + b_conv3)\n",
    "h_pool2 = max_pool_2x2(h_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_neurons = 512\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, fc_neurons])\n",
    "b_fc1 = bias_variable([fc_neurons])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fc_neurons, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin splitting\n",
      "Test data obtained\n",
      "Train data obtained\n",
      "step 0, training accuracy 0.033333\n",
      "step 4, training accuracy 1.000000\n",
      "step 8, training accuracy 1.000000\n",
      "step 12, training accuracy 1.000000\n",
      "test accuracy for fold 1 = 1\n",
      "Begin splitting\n",
      "Test data obtained\n",
      "Train data obtained\n",
      "step 0, training accuracy 0.666667\n",
      "step 4, training accuracy 1.000000\n",
      "step 8, training accuracy 1.000000\n",
      "step 12, training accuracy 1.000000\n",
      "test accuracy for fold 2 = 1\n",
      "Begin splitting\n",
      "Test data obtained\n",
      "Train data obtained\n",
      "step 0, training accuracy 0.000000\n",
      "step 4, training accuracy 1.000000\n",
      "step 8, training accuracy 1.000000\n",
      "step 12, training accuracy 1.000000\n",
      "test accuracy for fold 3 = 1\n",
      "Begin splitting\n",
      "Test data obtained\n",
      "Train data obtained\n",
      "step 0, training accuracy 1.000000\n",
      "step 4, training accuracy 1.000000\n",
      "step 8, training accuracy 1.000000\n",
      "step 12, training accuracy 1.000000\n",
      "test accuracy for fold 4 = 1\n",
      "Begin splitting\n",
      "Test data obtained\n",
      "Train data obtained\n",
      "step 0, training accuracy 0.900000\n",
      "step 4, training accuracy 1.000000\n",
      "step 8, training accuracy 1.000000\n",
      "step 12, training accuracy 1.000000\n",
      "test accuracy for fold 5 = 1\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "subset_size = len(data) / 5\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    current_batch_ind = 0\n",
    "    print 'Begin splitting'\n",
    "    test_data = data[fold*subset_size:(fold+1)*subset_size]\n",
    "    test_data = test_data.reset_index(drop = True)\n",
    "    print 'Test data obtained'\n",
    "    \n",
    "    train_data = pd.concat([data[:fold*subset_size], data[(fold+1)*subset_size:]])\n",
    "    train_data = train_data.reset_index(drop = True)\n",
    "    print 'Train data obtained'\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for i in range(16):\n",
    "        batch, empty = next_batch(30, train_data)\n",
    "        if empty:\n",
    "            break\n",
    "        if i%4 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %f\"%(i, train_accuracy))\n",
    "            #print(\"1 - %g\"%(batch[1].mean()) )\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    current_batch_ind = 0\n",
    "    cv_batch, _ = next_batch(subset_size, test_data)\n",
    "    print(\"test accuracy for fold %d = %g\"%(fold+1, accuracy.eval(feed_dict={\n",
    "    x: cv_batch[0], y_: cv_batch[1], keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    batch, empty = next_batch(30, data)\n",
    "    if empty:\n",
    "        break\n",
    "    if i%1 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %f\"%(i, train_accuracy))\n",
    "        print(\"1 - %g\"%(batch[1].mean()) )\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "current_img_index = \n",
    "test_batch = next_batch(2000, test, mask_test, gt_test)\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: test_batch[0], y_: test_batch[1], keep_prob: 1.0}))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = data[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = a.reset_index(drop = True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
